{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n5Mp9VjgXr6"
   },
   "source": [
    "ResNet18, 34, 50, VIP, MVP, DINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to move the loading into the Compiled Models Notebook From Here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 12286,
     "status": "ok",
     "timestamp": 1724873465721,
     "user": {
      "displayName": "Yash Yardi",
      "userId": "13835457492308801462"
     },
     "user_tz": 300
    },
    "id": "o0pddPNzBli6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/scratch/ankile/miniforge3/envs/semrep/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import zarr\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.generate_embeddings import generate_embeddings\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.encoders import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcQYuFT0CtOr"
   },
   "source": [
    "**All Files Loading Info**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1724873528725,
     "user": {
      "displayName": "Yash Yardi",
      "userId": "13835457492308801462"
     },
     "user_tz": 300
    },
    "id": "-IpTw4Ua3VFK"
   },
   "outputs": [],
   "source": [
    "# NOTE: Change this to wherever the project is located\n",
    "project_root = Path(\"..\")\n",
    "\n",
    "data_path = project_root / \"data\" / \"processed\"\n",
    "models_path = project_root / \"models\"\n",
    "\n",
    "# Output path\n",
    "output_path = project_root / \"embeddings\" / \"encoders\"\n",
    "output_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16988,
     "status": "ok",
     "timestamp": 1724873545710,
     "user": {
      "displayName": "Yash Yardi",
      "userId": "13835457492308801462"
     },
     "user_tz": 300
    },
    "id": "F5jW4XG5CqO9",
    "outputId": "66d6ebe4-47b3-4908-98a8-bebfa079376e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 trajectories containing 24131 frames of sim data\n",
      "Loaded 50 trajectories containing 27699 frames of real data\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "sim = zarr.open(data_path / \"one_leg_med_sim.zarr\", mode=\"r\")\n",
    "real = zarr.open(data_path / \"one_leg_low_real.zarr\", mode=\"r\")\n",
    "\n",
    "datasets = {\"sim\": sim, \"real\": real}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    imgs = dataset[\"color_image2\"]\n",
    "    labels = dataset[\"action/pos\"]\n",
    "\n",
    "    print(\n",
    "        f\"Loaded {len(dataset['episode_ends'])} trajectories containing {imgs.shape[0]} frames of {name} data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "executionInfo": {
     "elapsed": 10375,
     "status": "error",
     "timestamp": 1724871452508,
     "user": {
      "displayName": "Samuel Biruduganti",
      "userId": "10120648566470628875"
     },
     "user_tz": 300
    },
    "id": "roNceVh8DpwC",
    "outputId": "f187ea27-84a7-49b2-c9d1-97b73758aea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if visualize:\n",
    "    # Sample 8 images from each dataset\n",
    "    sim_indices = np.random.choice(datasets[\"sim\"][\"color_image2\"].shape[0], size=8, replace=False)\n",
    "    real_indices = np.random.choice(datasets[\"real\"][\"color_image2\"].shape[0], size=8, replace=False)\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "\n",
    "    # Display the sampled images\n",
    "    for i, idx in enumerate(sim_indices):\n",
    "        axes[0, i].imshow(datasets[\"sim\"][\"color_image2\"][idx])\n",
    "        axes[0, i].axis(\"off\")\n",
    "\n",
    "    for i, idx in enumerate(real_indices):\n",
    "        axes[1, i].imshow(datasets[\"real\"][\"color_image2\"][idx])\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(m, dataset, dataset_type, num_samples=None, batch_size=1024):\n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings(\n",
    "        m,\n",
    "        dataset[\"color_image2\"],\n",
    "        num_samples=num_samples,\n",
    "        batch_size=batch_size,\n",
    "    ).numpy()\n",
    "\n",
    "    print(f\"Generated {embeddings.shape[0]} embeddings for {dataset_type} data\")\n",
    "    print(f\"Embedding shape: {embeddings.shape}\")\n",
    "    print(embeddings.min(), embeddings.max(), embeddings.mean(), embeddings.std())\n",
    "\n",
    "    # Make a split array for the embeddings into train and eval according to 90/10 split of trajectories\n",
    "    split_index = dataset[\"episode_ends\"][-5]\n",
    "\n",
    "    # Split the embeddings into train and eval\n",
    "    train_flag = np.zeros(embeddings.shape[0], dtype=bool)\n",
    "    train_flag[:split_index] = True\n",
    "\n",
    "    # Create a flag to indicate the dataset type (sim or real)\n",
    "    dataset_flag = np.full(embeddings.shape[0], dataset_type, dtype=str)\n",
    "\n",
    "    return embeddings, dataset[\"action/pos\"], train_flag, dataset_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_models_and_datasets(models, datasets, batch_size=1024):\n",
    "    results = {}\n",
    "\n",
    "    print(\"Available models:\", models.keys())\n",
    "\n",
    "    for model_name, model_class in models.items():\n",
    "        print(f\"Processing with {model_name}\")\n",
    "\n",
    "        try:\n",
    "            m: torch.nn.Module = model_class()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Process sim data\n",
    "        sim_embeddings, sim_labels, sim_train_flag, sim_dataset_flag = process_dataset(\n",
    "            m, datasets[\"sim\"], \"sim\", batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Process real data\n",
    "        real_embeddings, real_labels, real_train_flag, real_dataset_flag = process_dataset(\n",
    "            m, datasets[\"real\"], \"real\", batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Combine sim and real data\n",
    "        combined_embeddings = np.vstack((sim_embeddings, real_embeddings))\n",
    "        combined_labels = np.vstack((sim_labels, real_labels))\n",
    "        combined_train_flag = np.concatenate((sim_train_flag, real_train_flag))\n",
    "        combined_dataset_flag = np.concatenate((sim_dataset_flag, real_dataset_flag))\n",
    "\n",
    "        # Store results for this model\n",
    "        results = {\n",
    "            \"embeddings\": combined_embeddings,\n",
    "            \"labels\": combined_labels,\n",
    "            \"dataset_flag\": combined_dataset_flag,\n",
    "            \"train_flag\": combined_train_flag,\n",
    "        }\n",
    "\n",
    "        # Save the results\n",
    "        output_file = output_path / f\"{model_name}.npz\"\n",
    "        np.savez(\n",
    "            output_file,\n",
    "            **results,\n",
    "        )\n",
    "\n",
    "        print(f\"Finished processing {model_name}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_models = {\n",
    "    # \"mcr\": models[\"mcr\"]\n",
    "    # \"Swin\": models[\"Swin\"],\n",
    "    # \"BEiT\": models[\"BEiT\"],\n",
    "    # \"CoAtNet\": models[\"CoAtNet\"],\n",
    "    # \"vgg16\": models[\"vgg16\"],\n",
    "    # \"ResNet18\": models[\"ResNet18\"],\n",
    "    # \"ViT\": models[\"ViT\"],\n",
    "    # \"HybridViT\": models[\"HybridViT\"],\n",
    "    # \"VIP\": models[\"VIP\"],\n",
    "    # \"Swin\": models[\"Swin\"],\n",
    "    # \"CLIP-Base-16\": models[\"CLIP-Base-16\"]\n",
    "    **models,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: dict_keys(['ResNet18', 'ResNet34', 'ResNet50', 'ResNet101', 'EfficientNetB0', 'MobileNetv3', 'vgg16', 'vgg19', 'ViT', 'Swin', 'BEiT', 'DinoV2', 'VIP', 'R3M18', 'R3M34', 'R3M50', 'MCR', 'VC1-B', 'VC1-L', 'HRP-ResNet18', 'HRP-ViT', 'CLIP-Base-16', 'CLIP-Base-32', 'CLIP-Large-14'])\n",
      "Processing with ResNet18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing:  32%|███▏      | 30/95 [00:38<01:19,  1.23s/it]"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "\n",
    "all_results = process_all_models_and_datasets(process_models, datasets, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Essential Usage Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/ubuntu/semrep/embeddings/encoders/dinov2.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/ubuntu/semrep/embeddings/encoders/dinov2.npz\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloadsplit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_and_use_existing_split\n\u001b[0;32m----> 6\u001b[0m train_data, val_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_and_use_existing_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Now you can access the train and validation data like this:\u001b[39;00m\n\u001b[1;32m      9\u001b[0m train_embeddings \u001b[38;5;241m=\u001b[39m train_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/data/scratch/ankile/Survey-of-Pre-Trained-Vision-Encoders/src/models/loadsplit.py:17\u001b[0m, in \u001b[0;36mload_and_use_existing_split\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_use_existing_split\u001b[39m(file_path):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Load the NPZ file\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Extract the arrays\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# embeddings for model\u001b[39;00m\n",
      "File \u001b[0;32m/data/scratch/ankile/miniforge3/envs/semrep/lib/python3.10/site-packages/numpy/lib/_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    457\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    460\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/ubuntu/semrep/embeddings/encoders/dinov2.npz'"
     ]
    }
   ],
   "source": [
    "# Usage example for getting the size of training and val sets\n",
    "file_path = \"/home/ubuntu/semrep/embeddings/encoders/dinov2.npz\"\n",
    "\n",
    "from src.models.loadsplit import load_and_use_existing_split\n",
    "\n",
    "train_data, val_data = load_and_use_existing_split(file_path)\n",
    "\n",
    "# Now you can access the train and validation data like this:\n",
    "train_embeddings = train_data[\"embeddings\"]\n",
    "train_labels = train_data[\"labels\"]\n",
    "train_dataset_flag = train_data[\"dataset_flag\"]\n",
    "\n",
    "val_embeddings = val_data[\"embeddings\"]\n",
    "val_labels = val_data[\"labels\"]\n",
    "val_dataset_flag = val_data[\"dataset_flag\"]\n",
    "\n",
    "# Print some information about the splits\n",
    "print(f\"Training set size: {len(train_embeddings)}\")\n",
    "print(f\"Validation set size: {len(val_embeddings)}\")\n",
    "\n",
    "# Check the distribution of sim vs real data in each split\n",
    "print(\n",
    "    f\"Training set - Sim data: {np.sum(train_dataset_flag == 's')}, Real data: {np.sum(train_dataset_flag == 'r')}\"\n",
    ")\n",
    "print(\n",
    "    f\"Validation set - Sim data: {np.sum(val_dataset_flag == 's')}, Real data: {np.sum(val_dataset_flag == 'r')}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of selecting all embeddings instead of the split\n",
    "dataset = np.load(file_path)\n",
    "embeddings = dataset[\"embeddings\"]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
