{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6n5Mp9VjgXr6"
   },
   "source": [
    "ResNet18, 34, 50, VIP, MVP, DINO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We need to move the loading into the Compiled Models Notebook From Here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 12286,
     "status": "ok",
     "timestamp": 1724873465721,
     "user": {
      "displayName": "Yash Yardi",
      "userId": "13835457492308801462"
     },
     "user_tz": 300
    },
    "id": "o0pddPNzBli6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import zarr\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.models.generate_embeddings import generate_embeddings\n",
    "\n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.encoders import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TcQYuFT0CtOr"
   },
   "source": [
    "**All Files Loading Info**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1724873528725,
     "user": {
      "displayName": "Yash Yardi",
      "userId": "13835457492308801462"
     },
     "user_tz": 300
    },
    "id": "-IpTw4Ua3VFK"
   },
   "outputs": [],
   "source": [
    "# NOTE: Change this to wherever the project is located\n",
    "project_root = Path(\"..\")\n",
    "\n",
    "data_path = project_root / \"data\" / \"processed\"\n",
    "models_path = project_root / \"models\"\n",
    "\n",
    "# Output path\n",
    "output_path = project_root / \"embeddings\" / \"encoders\"\n",
    "output_path.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16988,
     "status": "ok",
     "timestamp": 1724873545710,
     "user": {
      "displayName": "Yash Yardi",
      "userId": "13835457492308801462"
     },
     "user_tz": 300
    },
    "id": "F5jW4XG5CqO9",
    "outputId": "66d6ebe4-47b3-4908-98a8-bebfa079376e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 50 trajectories containing 24131 frames of sim data\n",
      "Loaded 50 trajectories containing 27699 frames of real data\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "sim = zarr.open(data_path / \"one_leg_med_sim.zarr\", mode=\"r\")\n",
    "real = zarr.open(data_path / \"one_leg_low_real.zarr\", mode=\"r\")\n",
    "\n",
    "datasets = {\"sim\": sim, \"real\": real}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    imgs = dataset[\"color_image2\"]\n",
    "    labels = dataset[\"action/pos\"]\n",
    "\n",
    "    print(\n",
    "        f\"Loaded {len(dataset['episode_ends'])} trajectories containing {imgs.shape[0]} frames of {name} data\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 653
    },
    "executionInfo": {
     "elapsed": 10375,
     "status": "error",
     "timestamp": 1724871452508,
     "user": {
      "displayName": "Samuel Biruduganti",
      "userId": "10120648566470628875"
     },
     "user_tz": 300
    },
    "id": "roNceVh8DpwC",
    "outputId": "f187ea27-84a7-49b2-c9d1-97b73758aea2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if visualize:\n",
    "    # Sample 8 images from each dataset\n",
    "    sim_indices = np.random.choice(datasets[\"sim\"][\"color_image2\"].shape[0], size=8, replace=False)\n",
    "    real_indices = np.random.choice(\n",
    "        datasets[\"real\"][\"color_image2\"].shape[0], size=8, replace=False\n",
    "    )\n",
    "\n",
    "    # Create a figure and axes\n",
    "    fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n",
    "\n",
    "    # Display the sampled images\n",
    "    for i, idx in enumerate(sim_indices):\n",
    "        axes[0, i].imshow(datasets[\"sim\"][\"color_image2\"][idx])\n",
    "        axes[0, i].axis(\"off\")\n",
    "\n",
    "    for i, idx in enumerate(real_indices):\n",
    "        axes[1, i].imshow(datasets[\"real\"][\"color_image2\"][idx])\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(m, dataset, dataset_type, num_samples=None, batch_size=1024):\n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings(\n",
    "        m,\n",
    "        dataset[\"color_image2\"],\n",
    "        num_samples=num_samples,\n",
    "        batch_size=batch_size,\n",
    "    ).numpy()\n",
    "\n",
    "    print(f\"Generated {embeddings.shape[0]} embeddings for {dataset_type} data\")\n",
    "    print(f\"Embedding shape: {embeddings.shape}\")\n",
    "    print(embeddings.min(), embeddings.max(), embeddings.mean(), embeddings.std())\n",
    "\n",
    "    # Make a split array for the embeddings into train and eval according to 90/10 split of trajectories\n",
    "    split_index = dataset[\"episode_ends\"][-5]\n",
    "\n",
    "    # Split the embeddings into train and eval\n",
    "    train_flag = np.zeros(embeddings.shape[0], dtype=bool)\n",
    "    train_flag[:split_index] = True\n",
    "\n",
    "    # Create a flag to indicate the dataset type (sim or real)\n",
    "    dataset_flag = np.full(embeddings.shape[0], dataset_type, dtype=str)\n",
    "\n",
    "    return embeddings, dataset[\"action/pos\"], train_flag, dataset_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_models_and_datasets(models, datasets, batch_size=1024, overwrite=False):\n",
    "    results = {}\n",
    "\n",
    "    print(\"Available models:\", models.keys())\n",
    "\n",
    "    for model_name, model_class in models.items():\n",
    "        print(f\"Processing with {model_name}\")\n",
    "\n",
    "        # Check if the output file already exists\n",
    "        output_file = output_path / f\"{model_name}.npz\"\n",
    "\n",
    "        if output_file.exists() and not overwrite:\n",
    "            print(f\"Skipping {model_name} as the output file already exists\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            m: torch.nn.Module = model_class()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Process sim data\n",
    "        sim_embeddings, sim_labels, sim_train_flag, sim_dataset_flag = process_dataset(\n",
    "            m, datasets[\"sim\"], \"sim\", batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Process real data\n",
    "        real_embeddings, real_labels, real_train_flag, real_dataset_flag = process_dataset(\n",
    "            m, datasets[\"real\"], \"real\", batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Combine sim and real data\n",
    "        combined_embeddings = np.vstack((sim_embeddings, real_embeddings))\n",
    "        combined_labels = np.vstack((sim_labels, real_labels))\n",
    "        combined_train_flag = np.concatenate((sim_train_flag, real_train_flag))\n",
    "        combined_dataset_flag = np.concatenate((sim_dataset_flag, real_dataset_flag))\n",
    "\n",
    "        # Store results for this model\n",
    "        results = {\n",
    "            \"embeddings\": combined_embeddings,\n",
    "            \"labels\": combined_labels,\n",
    "            \"dataset_flag\": combined_dataset_flag,\n",
    "            \"train_flag\": combined_train_flag,\n",
    "        }\n",
    "\n",
    "        # Save the results\n",
    "        np.savez(\n",
    "            output_file,\n",
    "            **results,\n",
    "        )\n",
    "\n",
    "        print(f\"Finished processing {model_name}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_models = dict(\n",
    "    # \"mcr\": models[\"mcr\"]\n",
    "    # \"Swin\": models[\"Swin\"],\n",
    "    # \"BEiT\": models[\"BEiT\"],\n",
    "    # \"CoAtNet\": models[\"CoAtNet\"],\n",
    "    # \"vgg16\": models[\"vgg16\"],\n",
    "    # \"ResNet18\": models[\"ResNet18\"],\n",
    "    # \"ViT\": models[\"ViT\"],\n",
    "    # \"HybridViT\": models[\"HybridViT\"],\n",
    "    # \"VIP\": models[\"VIP\"],\n",
    "    # \"Swin\": models[\"Swin\"],\n",
    "    # \"CLIP-Base-16\": models[\"CLIP-Base-16\"]\n",
    "    # list(models.items())[0:12]\n",
    "    # VIP=models[\"VIP\"]\n",
    "    # MVP=models[\"MVP\"]\n",
    "    # **models\n",
    "    [(key, value) for key, value in models.items() if key == \"VC1-B\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: dict_keys(['VC1-B'])\n",
      "Processing with VC1-B\n",
      "Skipping VC1-B as the output file already exists\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 256\n",
    "overwrite = False\n",
    "\n",
    "all_results = process_all_models_and_datasets(\n",
    "    process_models, datasets, batch_size=batch_size, overwrite=overwrite\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "sr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
