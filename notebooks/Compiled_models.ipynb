{"cells":[{"cell_type":"markdown","metadata":{"id":"6n5Mp9VjgXr6"},"source":["ResNet18, 34, 50, VIP, MVP, DINO\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n","%autoreload 2"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":12286,"status":"ok","timestamp":1724873465721,"user":{"displayName":"Yash Yardi","userId":"13835457492308801462"},"user_tz":300},"id":"o0pddPNzBli6"},"outputs":[],"source":["from pathlib import Path\n","\n","import zarr\n","\n","import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Check if CUDA is available and set the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","print(\"Device:\", device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torchvision.transforms as transforms\n","from r3m import load_r3m\n","import mvp\n","from vip import load_vip"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from src.models.encoders import models"]},{"cell_type":"markdown","metadata":{"id":"TcQYuFT0CtOr"},"source":["**All Files Loading Info**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1724873528725,"user":{"displayName":"Yash Yardi","userId":"13835457492308801462"},"user_tz":300},"id":"-IpTw4Ua3VFK"},"outputs":[],"source":["# NOTE: Change this to wherever the project is located in your google drive\n","project_root = Path(\".\").resolve().parent\n","# project_root = Path(\"/content/drive/MyDrive/17 â€“ Research/sim-2-real-representation-learning\") # Lars\n","data_path = project_root / \"data\" / \"processed\"\n","models_path = project_root / \"models\"\n","embeddings_path = project_root / \"embeddings\"\n","\n","print(\"Project root:\", project_root)\n","print(\"Data path:\", data_path)\n","print(\"Models path:\", models_path)\n","print(\"Embeddings path:\", embeddings_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16988,"status":"ok","timestamp":1724873545710,"user":{"displayName":"Yash Yardi","userId":"13835457492308801462"},"user_tz":300},"id":"F5jW4XG5CqO9","outputId":"66d6ebe4-47b3-4908-98a8-bebfa079376e"},"outputs":[],"source":["# Load the data\n","sim1 = zarr.open(data_path / \"one_leg_low_sim.zarr\", mode=\"r\")\n","sim2 = zarr.open(data_path / \"one_leg_med_sim.zarr\", mode=\"r\")\n","real = zarr.open(data_path / \"one_leg_low_real.zarr\", mode=\"r\")\n","sim1_imgs = sim1[\"color_image2\"]\n","sim2_imgs = sim2[\"color_image2\"]\n","real_imgs = real[\"color_image2\"]\n","sim1_labels = sim1[\"action/pos\"]\n","sim2_labels = sim2[\"action/pos\"]\n","real_labels = real[\"action/pos\"]\n","\n","\n","print(f\"Loaded {len(sim1['episode_ends'])} trajectories containing {sim1_imgs.shape[0]} frames\")\n","print(f\"Loaded {len(sim2['episode_ends'])} trajectories containing {sim2_imgs.shape[0]} frames\")\n","print(f\"Loaded {len(real['episode_ends'])} trajectories containing {real_imgs.shape[0]} frames\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":653},"executionInfo":{"elapsed":10375,"status":"error","timestamp":1724871452508,"user":{"displayName":"Samuel Biruduganti","userId":"10120648566470628875"},"user_tz":300},"id":"roNceVh8DpwC","outputId":"f187ea27-84a7-49b2-c9d1-97b73758aea2"},"outputs":[],"source":["# Sample 8 images from each dataset\n","sim1_indices = np.random.choice(sim1_imgs.shape[0], size=8, replace=False)\n","sim2_indices = np.random.choice(sim2_imgs.shape[0], size=8, replace=False)\n","real_indices = np.random.choice(real_imgs.shape[0], size=8, replace=False)\n","\n","# Create a figure and axes\n","fig, axes = plt.subplots(2, 8, figsize=(20, 5))\n","\n","# # Display the sampled images\n","# for i, idx in enumerate(sim1_indices):\n","#     axes[0, i].imshow(sim1_imgs[idx])\n","#     axes[0, i].axis(\"off\")\n","\n","for i, idx in enumerate(sim2_indices):\n","    axes[1 - 1, i].imshow(sim2_imgs[idx])\n","    axes[1 - 1, i].axis(\"off\")\n","\n","for i, idx in enumerate(real_indices):\n","    axes[2 - 1, i].imshow(real_imgs[idx])\n","    axes[2 - 1, i].axis(\"off\")\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9WLbZuZRDzy2"},"source":["**Load Models**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"elapsed":224,"status":"error","timestamp":1724880381680,"user":{"displayName":"Yash Yardi","userId":"13835457492308801462"},"user_tz":300},"id":"gxaJdMt8gH5_","outputId":"96f8d6a8-007e-4e74-ea2a-2e192acd20c6"},"outputs":[],"source":["# Choose the model to use\n","selected_model_name = \"ResNet18\"  # Change this to the desired model's name\n","\n","# Load the selected model\n","model = models[selected_model_name]().cuda()\n","\n","model.eval()"]},{"cell_type":"markdown","metadata":{"id":"qa_4z6ZW4DLy"},"source":["### Example of loading our own custom weights\n","\n","I've trained some models on the data that we've visualized above (the real and the nicely rendered sets). Here, I'll show an example of how we can load up our own weights, and I'd be really curious to see how these embeddings might be different to the pre-trained encoders.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":879},"executionInfo":{"elapsed":311,"status":"error","timestamp":1724871858912,"user":{"displayName":"Samuel Biruduganti","userId":"10120648566470628875"},"user_tz":300},"id":"MkCk7Mk44cIK","outputId":"8adf7c77-41e1-4a9b-e36b-763fd93fe8ef"},"outputs":[],"source":["model_names = [\n","    \"confusion_chkpt_999.pt\",\n","    \"naive_chkpt_999.pt\",\n","    \"upwt_chkpt_999.pt\",\n","]\n","# Initialize the model to load the weights into\n","r3m = load_r3m(\"resnet18\").cuda().eval().module\n","\n","# Read in the state dict from file\n","wt_path = models_path / \"cotraining\" / model_names[0]\n","state_dict = torch.load(wt_path)\n","\n","r3m.state_dict().keys()\n","\n","# Load the weights into the model, and we're ready to use it to predict embeddings\n","r3m.load_state_dict(state_dict)"]},{"cell_type":"markdown","metadata":{"id":"awgemoOgEzpI"},"source":["**Output Dimensions**\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":1207,"status":"ok","timestamp":1724873932911,"user":{"displayName":"Yash Yardi","userId":"13835457492308801462"},"user_tz":300},"id":"WFq__LybEVSU"},"outputs":[],"source":["def output():\n","    if selected_model_name == \"MVP\" or selected_model_name == \"VIP\":\n","        random_input = torch.rand((1, 3, 224, 224)).to(device)  # Move to the correct device\n","    else:\n","        random_input = (\n","            torch.randint(0, 255, size=(1, 3, 224, 224)).float().to(device) / 255.0\n","        )  # Convert to float and move to device\n","\n","    # Pass the random data through the encoder\n","    with torch.no_grad():\n","        output = model(random_input)\n","\n","    return output\n","\n","\n","model_output_dim = output()"]},{"cell_type":"markdown","metadata":{"id":"VHlGNizOcsPJ"},"source":["**TSNE**\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":524705,"status":"error","timestamp":1724874623102,"user":{"displayName":"Yash Yardi","userId":"13835457492308801462"},"user_tz":300},"id":"oBpoKmk6EfYG","outputId":"e593e6c6-5e7b-4e14-8e6b-0a98da27930b"},"outputs":[],"source":["from src.models.tsne import plot_tsne\n","\n","file_path = embeddings_path / \"encoders/dinov2.npz\"\n","\n","# Sample a subset of the data\n","n_samples = 1000\n","dataset = np.load(file_path)\n","\n","\n","indices = np.random.choice(len(dataset[\"embeddings\"]), n_samples, replace=False)\n","\n","embeddings = dataset[\"embeddings\"][indices]\n","domain_labels = dataset[\"dataset_flag\"][indices]\n","\n","# Plot t-SNE\n","plot_tsne(embeddings, domain_labels)"]},{"cell_type":"markdown","metadata":{},"source":["**IMAGE FINDER AND Grad-CAM**\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","from torchvision.transforms import Normalize\n","from pytorch_grad_cam import GradCAM\n","from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n","from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n","from pytorch_grad_cam.utils.image import show_cam_on_image\n","\n","model = model\n","model.eval()\n","\n","target_layer = model.layer4[-1]\n","\n","# Initialize Grad-CAM\n","cam = GradCAM(model=model, target_layers=[target_layer], use_cuda=torch.cuda.is_available())\n","\n","# Select one image from the sim1 dataset\n","image_index = 0  # You can change this to select different images\n","input_image = sim1_images[image_index]\n","\n","# Preprocess the image (assuming normalization is needed for the encoder)\n","# Define your normalization parameters based on how the model was trained\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","preprocessed_img = preprocess_image(input_image.cpu().numpy(), mean=mean, std=std)\n","\n","target_class = 0\n","grayscale_cam = cam(input_tensor=preprocessed_img, targets=[ClassifierOutputTarget(target_class)])\n","\n","input_image_np = input_image.cpu().numpy().transpose(1, 2, 0)\n","input_image_np = (input_image_np - np.min(input_image_np)) / (\n","    np.max(input_image_np) - np.min(input_image_np)\n",")\n","\n","\n","heatmap_image = show_cam_on_image(input_image_np, grayscale_cam, use_rgb=True)\n","\n","plt.figure(figsize=(10, 5))\n","plt.subplot(1, 2, 1)\n","plt.imshow(input_image_np)\n","plt.title(\"Original Image\")\n","plt.subplot(1, 2, 2)\n","plt.imshow(heatmap_image)\n","plt.title(\"Grad-CAM Heatmap\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["**Domain Probing**\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from src.models.domainprobe import train_domain_probe\n","\n","file_path = embeddings_path / \"encoders/dinov2.npz\"\n","\n","train_domain_probe(file_path, epochs=30, batch_size=200, lr=0.001)"]},{"cell_type":"markdown","metadata":{},"source":["**Domain Labels Alternative**\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from src.models.domain_labels_r2 import domain_r_squared\n","from src.models.loadsplit import load_and_use_existing_split\n","\n","file_path = \"/home/ubuntu/semrep/embeddings/encoders/dinov2.npz\"\n","\n","\n","train_data, val_data = load_and_use_existing_split(file_path)\n","\n","dataset = np.load(file_path)\n","embeddings = dataset[\"embeddings\"]\n","\n","domain_r_squared(embeddings, dataset[\"dataset_flag\"])"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.0b3"}},"nbformat":4,"nbformat_minor":0}
